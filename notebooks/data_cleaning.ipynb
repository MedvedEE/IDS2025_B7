{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Data cleaning\n","# Two datasets: International Soil Moisture Network (ISMN) and Estonian Environment Agency's historical weather data."],"metadata":{"id":"S_wADOfFEsoy"}},{"cell_type":"markdown","source":["**ISMN data cleaning**"],"metadata":{"id":"j4-TAcBNMSRh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"09256718"},"outputs":[],"source":["###script to group files to folders based on the availability of all variables\n","###example:\n","#folder_with_p_and_ta\n","#folder_with_p_and_ta_and_sm\n","\n","import os\n","import shutil\n","\n","\n","base_dir = r\"/path/to/your/folder\"\n","output_dir = r\"/path/to/your/folder\"\n","os.makedirs(output_dir, exist_ok=True)\n","keywords = {\n","    \"_sm_\": \"sm\",\n","    \"_ta_\": \"ta\",\n","    \"_p_\": \"p\"}\n","\n","def extract_keywords(filename, keywords):\n","    name_lower = filename.lower()\n","    return [kw for kw in keywords if kw.lower() in name_lower]\n","\n","for root, dirs, files in os.walk(base_dir):\n","\n","    if os.path.abspath(root) == os.path.abspath(output_dir):\n","        continue\n","\n","    # Find CSV files\n","    stm_files = [f for f in files if f.lower().endswith(\".stm\")]\n","\n","    if not stm_files:\n","        continue\n","\n","    # Collect keywords found inside this folder\n","    detected_keywords = set()\n","\n","    for file in stm_files:\n","        found = extract_keywords(file, keywords)\n","        detected_keywords.update(found)\n","\n","    # If no keywords were found at all, skip\n","    if not detected_keywords:\n","        continue\n","\n","    # Sort keywords to create consistent folder names\n","    keywords_sorted = sorted(detected_keywords)\n","\n","    # Build folder name\n","    folder_name = \"Folder_with_\" + \"_and_\".join(keywords_sorted)\n","\n","    # Create the output folder\n","    new_folder_path = os.path.join(output_dir, folder_name)\n","    os.makedirs(new_folder_path, exist_ok=True)\n","\n","    # Copy matching files\n","    for stm in stm_files:\n","        src = os.path.join(root, stm)\n","        dst = os.path.join(new_folder_path, stm)\n","        shutil.copy2(src, dst)\n","\n","    print(f\"Created: {new_folder_path}\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9f3d00d"},"outputs":[],"source":["###STM_TO_CSV convert####\n","\n","import os\n","import pandas as pd\n","from tqdm import tqdm\n","\n","def process_stm_file(filepath, filename):\n","    \"\"\"\n","    Reads a .stm file, extracts metadata (first line), and processes the remaining content\n","    into a DataFrame with predefined headers.\n","    \"\"\"\n","    data = []\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        lines = f.readlines()\n","\n","    if not lines:\n","        return None, None  # Return None if file is empty\n","\n","    metadata = lines[0].strip()\n","\n","    for line in lines[1:]:  # Skip the first line (metadata)\n","        line = line.strip()\n","        if not line:\n","            continue  # Skip empty lines\n","        row = [cell for cell in line.split(\" \") if cell != \"\"]  # Split on spaces, remove empty values\n","        data.append(row)\n","\n","    df = pd.DataFrame(data)\n","    if \"_sm_\" in filename:\n","        value_header = \"value (m3*m3)\"\n","    elif \"_ta_\" in filename:\n","        value_header = \"value (C)\"\n","    elif \"_p_\" in filename:\n","        value_header = \"value (mm)\"\n","    else:\n","        raise ValueError(f\"Unknown file in {filename}\")\n","\n","\n","    headers = [\"date\", \"time\", value_header, \"quality_flag\", \"source_flag\"]\n","\n","    while df.shape[1] < len(headers):\n","        df[df.shape[1]] = None  # Fill missing columns with NaN\n","\n","    df.columns = headers\n","    return df, metadata\n","\n","def count_total_stm_files(root_folder):\n","    \"\"\"Counts the total number of .stm files across all subfolders in the root folder.\"\"\"\n","    total_files = 0\n","    for dirpath, _, filenames in os.walk(root_folder):\n","        total_files += len([f for f in filenames if f.endswith(\".stm\")])\n","    return total_files\n","\n","def convert_stm_folders(root_folder, output_folder):\n","    \"\"\"\n","    Processes all .stm files in the root folder and its subfolders, converting them to csv format.\n","    \"\"\"\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","    total_files = count_total_stm_files(root_folder)\n","\n","    with tqdm(total=total_files, desc=\"Converting .stm files\") as pbar:\n","        for dirpath, _, filenames in os.walk(root_folder):\n","            relative_path = os.path.relpath(dirpath, root_folder)\n","            subfolder_output_path = os.path.join(output_folder, relative_path)\n","            if not os.path.exists(subfolder_output_path):\n","                os.makedirs(subfolder_output_path)\n","\n","            for filename in filenames:\n","                if filename.endswith(\".stm\"):\n","                    stm_path = os.path.join(dirpath, filename)\n","                    output_csv = os.path.join(subfolder_output_path, f\"{os.path.splitext(filename)[0]}.csv\")\n","\n","                    df, metadata = process_stm_file(stm_path, filename)\n","                    if df is None:\n","                        print(f\"Skipping empty file: {filename}\")\n","                        pbar.update(1)\n","                        continue\n","\n","                                        # Save main data to CSV\n","                    df.to_csv(output_csv, index=False)\n","\n","                    # Save metadata to a separate CSV if it exists\n","                    if metadata:\n","                        metadata_filename = f\"{os.path.splitext(filename)[0]}_metadata.csv\"\n","                        metadata_path = os.path.join(metadata_output_folder, metadata_filename)\n","                        metadata_df = pd.DataFrame([{\"Metadata\": metadata}])\n","                        metadata_df.to_csv(metadata_path, index=False, header=False)\n","\n","                    pbar.update(1)\n","\n","# Define root input folder\n","root_folder = r\"/path/to/your/folder\"\n","output_folder = r\"/path/to/your/folder\"\n","\n","metadata_output_folder = os.path.join(root_folder, \"metadata_files\")\n","os.makedirs(metadata_output_folder, exist_ok=True)\n","\n","convert_stm_folders(root_folder, output_folder)\n","print(\"Conversion complete!\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eee42d92"},"outputs":[],"source":["### Cleaning prodeuced csv and calculating daily average\n","### If dubious or missing values, mean will be calculated from +-1 values\n","### only values with quality flag \"G\" --> good, accepted\n","\n","\n","import pandas as pd\n","import os\n","from tqdm import tqdm\n","\n","csv_folder = r\"/path/to/your/folder\"\n","output_folder = r\"/path/to/your/folder\"\n","os.makedirs(output_folder, exist_ok=True)\n","\n","files = [f for f in os.listdir(csv_folder) if f.endswith(\".csv\")]\n","#COSMOS-UK_COSMOS-UK_AliceHolt_sm_0.000000_0.300000_Cosmic-ray-Probe_20100101_20251105\n","for filename in tqdm(files, desc=\"Processing csv fikes\"):\n","    filepath = os.path.join(csv_folder, filename)\n","    df = pd.read_csv(filepath)\n","    pices = filename.split(\"_\")\n","    network = pices[1]\n","    station = pices[2]\n","    depth_from = pices[4]\n","    depth_to = pices[5]\n","    sensor = pices[6]\n","\n","    # Determine value column name based on filename\n","    if \"_sm_\" in filename:\n","        value_col = \"value (m3*m3)\"\n","    elif \"_ta_\" in filename:\n","        value_col = \"value (C)\"\n","    elif \"_p_\" in filename:\n","        value_col = \"value (mm)\"\n","    else:\n","        print(f\"Unknown value column for file: {filename}\")\n","        continue\n","\n","    # Replace -9999 with NaN\n","    df[value_col] = pd.to_numeric(df[value_col], errors='coerce')\n","    df[value_col] = df[value_col].replace(-9999, pd.NA)\n","\n","    # Combine date and time into datetime index\n","    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], errors='coerce')\n","    df = df.dropna(subset=['datetime'])  # Drop rows with invalid datetime\n","    df.set_index('datetime', inplace=True)\n","\n","    # Filter good quality values\n","    df_good = df[df['quality_flag'] == 'G']\n","\n","    # Daily mean from good values\n","    daily_good = df_good.resample('D')[value_col].mean()\n","\n","    # All days in original data\n","    all_days = df.resample('D')[value_col].mean()\n","    final_daily = []\n","\n","    for date in all_days.index:\n","        if pd.isna(daily_good.get(date)):\n","            # Fallback to ±1 month average\n","            start = date - pd.DateOffset(days=30)\n","            end = date + pd.DateOffset(days=30)\n","            window = df[(df.index >= start) & (df.index <= end)]\n","            fallback_mean = window[value_col].mean()\n","            rounded_value = round(fallback_mean, 4)\n","            final_daily.append((date.date(), rounded_value, \"fallback\"))\n","        else:\n","            rounded_value = round(daily_good[date], 4)\n","            final_daily.append((date.date(), rounded_value, \"good\"))\n","\n","\n","    # Save result\n","    result_df = pd.DataFrame(final_daily, columns=['date', 'daily_avg', 'source'])\n","    unit = value_col.split(\"(\")[-1].strip(\")\")\n","    result_df.rename(columns={value_col: f'daily_avg ({unit})'}, inplace=True)\n","\n","    ### adding network, station, depth and sensor data from filename\n","    result_df[\"network\"] = network\n","    result_df[\"station\"] = station\n","    result_df[\"depth_from\"] = depth_from\n","    result_df[\"depth_to\"] = depth_to\n","    result_df[\"sensor\"] = sensor\n","    output_path = os.path.join(output_folder, filename)\n","    result_df.to_csv(output_path, index=False)\n","\n","print(\"All csv cleaned\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7e5edb05"},"outputs":[],"source":["\n","### CSV merge to precipitation, soilmoisture and airtemperature csv-s\n","\n","import os\n","import pandas as pd\n","from tqdm import tqdm\n","\n","def merge_csv_files(input_folder, output_folder):\n","    # Define the patterns to search for in filenames\n","    patterns = [\"_sm_\", \"_p_\", \"_ta_\", \"_ts_\"]\n","\n","    # Create a dictionary to hold DataFrames for each pattern\n","    dfs = {pattern: [] for pattern in patterns}\n","\n","    # Get the list of all CSV files in the input folder and subfolders\n","    csv_files = [os.path.join(root, file)\n","                 for root, _, files in os.walk(input_folder)\n","                 for file in files if file.endswith(\".csv\")]\n","\n","    # Iterate over files with a progress bar\n","    for file_path in tqdm(csv_files, desc=\"Merging CSV files\"):\n","        try:\n","            # Read the CSV file\n","            df = pd.read_csv(file_path, encoding='utf-8')\n","\n","            # Check which pattern the file matches and add to the corresponding list\n","            for pattern in patterns:\n","                if pattern in file_path:\n","                    dfs[pattern].append(df)\n","                    break\n","        except Exception as e:\n","            print(f\"Error reading file {file_path}: {e}\")\n","\n","    # Merge DataFrames for each pattern and save to separate CSV files\n","    for pattern, df_list in dfs.items():\n","        if df_list:\n","            merged_df = pd.concat(df_list, ignore_index=True)\n","            output_file = os.path.join(output_folder, f\"merged_{pattern.strip('_')}.csv\")\n","            try:\n","                merged_df.to_csv(output_file, index=False, encoding='utf-8')\n","                print(f\"Merged {len(df_list)} files into {output_file}\")\n","            except Exception as e:\n","                print(f\"Error writing file {output_file}: {e}\")\n","\n","# Define input and output folders\n","input_folder = r\"/path/to/your/folder\"\n","output_folder = r\"/path/to/your/folder\"\n","os.makedirs(output_folder, exist_ok=True)\n","\n","# Create output folder if it doesn't exist\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","# Merge CSV files\n","merge_csv_files(input_folder, output_folder)\n","print(\"Merging complete!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47838fa4"},"outputs":[],"source":["### Creating inner join between previous sm, ta and p csv-s\n","### Creating date filter\n","\n","import pandas as pd\n","\n","df_sm = pd.read_csv(\"/path/to/your/file/merged_sm.csv\")\n","df_ta = pd.read_csv(\"/path/to/your/file/merged_ta.csv\")\n","df_p = pd.read_csv(\"/path/to/your/file/merged_p.csv\")\n","\n","df_sm.reset_index(drop=True, inplace=True)\n","df_ta.reset_index(drop=True, inplace=True)\n","df_p.reset_index(drop=True, inplace=True)\n","\n","\n","df_sm.rename(columns={\"daily_avg\": \"soil_moisture\",\n","                      \"depth_from\": \"sm_depth_from\",\n","                      \"depth_to\": \"sm_depth_to\",\n","                      \"source\": \"sm_source\",}, inplace=True)\n","df_ta.rename(columns={\"daily_avg\": \"temperature\",\n","                      \"source\": \"ta_source\",\n","                      \"depth_from\": \"ta_depth_from\",\n","                      \"depth_to\": \"ta_depth_to\",\n","                      \"sensor\": \"ta_sensor\"}, inplace=True)\n","df_p.rename(columns={\"daily_avg\": \"precipitation\",\n","                     \"source\": \"p_source\",\n","                      \"depth_from\": \"p_depth_from\",\n","                      \"depth_to\": \"p_depth_to\",\n","                      \"sensor\": \"p_sensor\"}, inplace=True)\n","\n","df_sm.drop([\"network\"], axis=1, inplace=True)\n","df_ta.drop([\"network\"], axis=1, inplace=True)\n","\n","\n","merged = pd.merge(df_sm, df_ta, on=[\"station\", \"date\"], how=\"inner\")\n","merged = pd.merge(merged, df_p, on=[\"station\", \"date\"], how=\"inner\")\n","\n","output = merged[merged[\"date\"] >= \"2018-01-01\"]\n","\n","\n","output.to_csv(\"/path/to/your/file/ISMN_data_clean.csv\", index=False)"]},{"cell_type":"code","source":["### ISMN sanity"],"metadata":{"id":"JZm0c5hDL7LW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3d934bb8"},"outputs":[],"source":["import pandas as pd\n","ismn_data = pd.read_csv(\"/path/to/your/file/ISMN_data_clean.csv\")\n","ismn_data.head()\n","ismn_data[\"network\"].unique()\n","ismn_data.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3a231b94"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","ismn_data['date'] = pd.to_datetime(ismn_data['date'])\n","\n","#Checking different stations or networks\n","year1 = ismn_data[(ismn_data[\"date\"] < \"2019-01-01\") & (ismn_data[\"network\"] == \"RSMN\")]\n","\n","plt.figure(figsize=(12, 6))\n","sns.lineplot(data=year1, x=\"date\", y=\"soil_moisture\", label=\"Soilmoisture\")\n","sns.lineplot(data=year1, x=\"date\", y=\"precipitation\", label=\"precipitation\")\n","sns.lineplot(data=year1, x=\"date\", y=\"temperature\", label=\"Air temperature\")\n","plt.xlabel(\"Year\")\n","plt.ylabel(\"SM, TA, P values\")\n","plt.title(\"ISMN data cleaned\")\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","source":["Estonian data cleaning"],"metadata":{"id":"srYgW-J4MHVT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MOSQW6wEqck"},"outputs":[],"source":["## Cleaning Estonian data\n","## Kert\n","\n","import pandas as pd\n","\n","df = pd.read_csv(\"Station_Name.csv\",\n","                 low_memory=False,\n","                 encoding=\"utf-8\",\n","                 sep=\";\",\n","                 header=1)\n","\n","df = df[[\n","    \"Aasta\",\n","    \"Kuu\",\n","    \"Päev\",\n","    \"Tunni sademete summa mm\",\n","    \"Õhutemperatuur °C\"\n","]]\n","\n","df['Õhutemperatuur °C'] = df['Õhutemperatuur °C'].str.replace(',', '.').astype(float)\n","df['Tunni sademete summa mm'] = df['Tunni sademete summa mm'].str.replace(',', '.').astype(float)\n","\n","first_valid_index = df['Tunni sademete summa mm'].first_valid_index()\n","df = df.loc[first_valid_index:]\n","\n","# Excluse empty and NaN values\n","df = df[df['Tunni sademete summa mm'].notna() & (df['Tunni sademete summa mm'] > 0)]\n","\n","# Daily average\n","df_daily = df.groupby(['Aasta','Kuu','Päev'], as_index=False).agg({\n","    'Tunni sademete summa mm':'mean',\n","    'Õhutemperatuur °C':'mean'\n","})\n","\n","# Transform dates to match ISMN data structure\n","df_daily['Kuupäev'] = pd.to_datetime(\n","    df_daily['Aasta'].astype(str) + '-' +\n","    df_daily['Kuu'].astype(str) + '-' +\n","    df_daily['Päev'].astype(str),\n","    format='%Y-%m-%d'\n",")\n","\n","df_daily_final = df_daily[['Kuupäev','Tunni sademete summa mm','Õhutemperatuur °C']]\n","\n","df_daily_final.to_csv(\"Station_Name_clean.csv\", index=False, sep=',', float_format='%.3f', encoding='utf-8-sig')\n","\n","print(df_daily_final.head())\n","\n","# Later found that forgot to change the attribute names (column names) to english language - this was performed on the main script."]},{"cell_type":"code","source":["###"],"metadata":{"id":"G244zp2QLkA5"},"execution_count":null,"outputs":[]}]}